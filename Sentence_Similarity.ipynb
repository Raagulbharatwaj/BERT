{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuInABOjYVh9PxycxKb2Al",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raagulbharatwaj/BERT/blob/main/Sentence_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Textual Similarity\n",
        "\n",
        "Semantic Textual Similarity is one of the major focuses of research in NLP tasks. Semantic Textual Similarity involves comparing two sentences semantically and finding the amount of similarity as a numeric measure.\n",
        "\n",
        "<br>\n",
        "\n",
        "Perhaps the most famous so far, and the current state-of-the-art in sentence similarity is done by German scientists Nils Reimers and Iryna Gurevych at\n",
        "Ubiquitous Knowledge Processing Lab (UKP-TUDA), Germany. In this notebook, we will explore how to get sentence representations from SentenceBERT.\n",
        "\n",
        "<br>\n",
        "\n",
        "You know the drill - install the Transformers library first!"
      ],
      "metadata": {
        "id": "k60pOZry95D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "9Fv-XfHjunaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3593cbf5-f94d-4050-e534-5bba63ece797"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One more install - this time for Sentence Transformers\n",
        "\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajm8zY5hOjGE",
        "outputId": "e9d68aea-0300-4366-ea22-2c97be1f1f64"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.22.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial imports\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "8zkzLmmaOBki"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's where the action starts. We have entered in the cell below, the sentences 'I am good' and 'I am well'. We will now use the sentenceBERT model to extract the embeddings of both sentences. Let's take a look."
      ],
      "metadata": {
        "id": "ARRdayV2tmyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"I am good\"\n",
        "sentence2 = \"I am well\"\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embedding1 = model.encode(sentence1)\n",
        "embedding2 = model.encode(sentence2)\n",
        "print(embedding1)\n",
        "print(embedding2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sVWrQYMOMs-",
        "outputId": "70ea6da4-1a4a-4e66-be28-20e5285ed3e6"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.44729328e-02 -6.41169325e-02 -1.38996672e-02  5.98502345e-03\n",
            " -6.86401129e-02  2.83300702e-04  1.08719945e-01  1.37069300e-02\n",
            " -7.01571479e-02 -1.43060843e-02  1.50692351e-02 -6.16868474e-02\n",
            " -1.59280393e-02 -7.89568468e-04  7.35849366e-02 -1.48512209e-02\n",
            "  1.59452222e-02 -4.52054664e-02 -1.02363721e-01 -1.09433578e-02\n",
            " -1.37880310e-01  4.54124212e-02  8.25411454e-02  2.70946249e-02\n",
            " -7.39210472e-02  2.41403212e-03 -9.94821358e-03  7.76677206e-03\n",
            "  6.58574775e-02 -3.51392068e-02 -8.45831335e-02  1.89159196e-02\n",
            "  4.44628373e-02 -2.09030528e-02 -5.26019298e-02  5.50589859e-02\n",
            " -3.32135893e-02 -9.28306505e-02  8.88481438e-02  3.54514681e-02\n",
            "  3.71231399e-02 -2.96136625e-02 -2.88637802e-02 -1.49481613e-02\n",
            "  7.98430236e-04  5.51729500e-02  4.45638504e-03  1.42977117e-02\n",
            "  5.29855900e-02 -6.63945153e-02 -6.60493150e-02  2.67059896e-02\n",
            " -5.40565588e-02  4.54549976e-02  9.22883675e-02  3.77403498e-02\n",
            " -2.00231113e-02 -1.32569717e-02  2.21695192e-02 -4.59340662e-02\n",
            "  7.94849172e-02 -2.66701775e-03 -2.01336220e-02 -7.68658891e-03\n",
            "  1.19274646e-01 -8.95910244e-03 -4.02867235e-02  2.87658833e-02\n",
            " -9.73944142e-02  3.94936185e-03 -1.29892416e-02 -3.82864140e-02\n",
            "  6.26183487e-03 -3.37874852e-02  3.64084058e-02  2.66052112e-02\n",
            "  3.44292969e-02 -3.33599113e-02  1.02487631e-01  9.51109156e-02\n",
            "  6.24753237e-02 -7.95921981e-02 -4.14748676e-02  1.79850252e-03\n",
            " -6.64867312e-02 -3.63857648e-03  3.62641662e-02  5.07944897e-02\n",
            " -1.55723123e-02  4.34843190e-02  2.62430981e-02  9.17956457e-02\n",
            " -4.64193476e-03  1.87774878e-02  2.85030203e-03  1.24978218e-02\n",
            "  6.60703108e-02 -1.28170904e-02 -6.08837418e-02  1.57058880e-01\n",
            "  7.37361312e-02  3.81696261e-02  4.85271513e-02 -3.63450609e-02\n",
            "  1.63136236e-02  2.08795108e-02 -1.35184797e-02  1.26044884e-01\n",
            " -2.24677585e-02 -6.75865933e-02 -1.05131082e-02 -1.36971874e-02\n",
            " -2.18837969e-02  1.04167536e-01  5.16774431e-02  7.12841749e-02\n",
            " -6.42207414e-02  3.24819982e-02 -1.31991440e-02  1.21666621e-02\n",
            " -8.61421693e-03  7.08715850e-03  6.86021820e-02  1.73380692e-02\n",
            " -7.82459229e-02 -2.43228897e-02  2.45732833e-02 -5.61600140e-33\n",
            "  7.69349858e-02  1.07796527e-02  9.87283885e-02  4.08842675e-02\n",
            " -7.66725764e-02  2.57804953e-02 -5.38936025e-03 -3.23855951e-02\n",
            " -2.13890914e-02 -5.41158346e-03 -2.13764366e-02  1.70909353e-02\n",
            " -4.47874609e-03  6.80809617e-02  5.37716523e-02  2.82917172e-02\n",
            "  2.60401033e-02 -3.91121814e-03  7.61165051e-03  8.91846940e-02\n",
            "  1.95135130e-03 -7.25653619e-02 -2.67477310e-03 -2.86567752e-04\n",
            " -3.37949805e-02 -1.03915380e-02 -9.37799364e-03 -1.19986102e-01\n",
            "  7.14891702e-02  1.43233426e-02 -7.22927898e-02  4.16206941e-02\n",
            " -1.72703639e-02  4.24951352e-02  2.05464363e-02 -4.21543866e-02\n",
            " -5.10479212e-02 -1.78758595e-02 -4.94550914e-02  3.86354327e-02\n",
            " -8.03588778e-02  1.31444968e-02 -1.54603012e-02 -7.91386738e-02\n",
            " -9.98166017e-03 -3.03419400e-02 -6.18697666e-02 -2.17910353e-02\n",
            " -8.85467082e-02  1.24021890e-02 -6.13946691e-02 -4.51318584e-02\n",
            " -2.08445732e-02 -6.10626377e-02 -1.59280337e-02  4.06183377e-02\n",
            " -4.87857126e-03  1.43387675e-01 -4.56180982e-02 -9.18120262e-04\n",
            "  6.46894574e-02  8.63333344e-02 -6.40518665e-02 -2.08223853e-02\n",
            " -1.62715927e-01  3.35116535e-02  4.08307016e-02 -4.51179668e-02\n",
            "  1.02094859e-01 -8.31719264e-02  2.81092599e-02  1.51357483e-04\n",
            "  1.44327832e-02  8.75879154e-02  8.27123299e-02  9.10481717e-03\n",
            " -8.36038515e-02  5.84186651e-02 -1.97886378e-02  2.68687829e-02\n",
            "  6.27127141e-02  3.06967590e-02 -6.16477728e-02 -3.84473130e-02\n",
            "  8.46700519e-02  1.07697761e-02 -5.61866313e-02  3.14054452e-02\n",
            "  4.48706634e-02  3.92812677e-02 -1.65027454e-02  6.57211617e-02\n",
            "  8.88344720e-02  3.79683338e-02 -7.00346157e-02  4.73791667e-33\n",
            "  9.42289606e-02  7.97550008e-02 -9.83452238e-03  3.62548679e-02\n",
            " -8.26781318e-02  4.58855880e-03  2.13117413e-02  8.51187333e-02\n",
            " -1.01947449e-01  5.52126728e-02  8.69085640e-03 -8.68069101e-03\n",
            "  2.21892167e-02 -7.66360259e-04  9.00727697e-03 -9.69248358e-03\n",
            "  6.45886809e-02  3.58943529e-02 -1.35392286e-02  3.15224789e-02\n",
            " -7.98041299e-02  5.57151251e-02 -9.80910584e-02  5.90333482e-03\n",
            "  4.92510910e-04  2.22922526e-02  1.35503327e-02 -1.45870317e-02\n",
            " -4.05409746e-02  1.30489096e-02  6.00150637e-02 -1.05994023e-01\n",
            " -5.43211661e-02 -1.17783779e-02  4.97261025e-02  7.49367625e-02\n",
            "  5.40740490e-02 -6.12627678e-02 -3.63765359e-02 -5.88227771e-02\n",
            "  3.57073210e-02  4.07321416e-02  9.33382753e-03  5.21392636e-02\n",
            " -2.83559319e-02 -1.18386196e-02  1.34213408e-02  2.82086562e-02\n",
            "  1.75196286e-02  3.51155810e-02  1.24481218e-02 -7.94420615e-02\n",
            " -2.97072008e-02  7.78727233e-03  1.46357445e-02  2.04739291e-02\n",
            " -2.34934110e-02 -5.99115603e-02  4.95772623e-02 -6.52435794e-02\n",
            " -6.90579265e-02  3.25114913e-02  1.81910209e-02  5.57644852e-02\n",
            "  7.40443692e-02 -4.69662920e-02 -6.56881034e-02  2.20804308e-02\n",
            " -6.23246357e-02 -2.23818086e-02 -2.54209992e-02  1.93331279e-02\n",
            " -3.15229706e-02 -6.12374656e-02  3.36157791e-02 -4.26208712e-02\n",
            " -1.38757341e-02 -1.77650563e-02 -4.51704524e-02 -9.04681012e-02\n",
            " -5.65217212e-02  6.16992787e-02  1.14477333e-02 -5.48085980e-02\n",
            " -3.56493406e-02 -5.94413988e-02  8.05177838e-02  5.36036938e-02\n",
            "  2.72676758e-02 -1.52939288e-02  4.25583757e-02  5.66585967e-03\n",
            "  1.16646383e-02 -5.52608371e-02  1.74807869e-02 -1.50793085e-08\n",
            "  3.65380347e-02  5.24832942e-02  8.82441550e-03 -1.27394432e-02\n",
            " -5.81708364e-02  5.43141030e-02 -1.09366871e-01 -5.01520000e-02\n",
            "  4.31723613e-03 -2.88524292e-02  3.40900533e-02 -1.44980326e-02\n",
            " -4.62957583e-02 -1.06967457e-01  1.07886508e-01  9.57118068e-03\n",
            " -2.04111729e-03  7.47541431e-03 -2.48172954e-02  3.09274644e-02\n",
            "  4.39583212e-02  4.33774702e-02 -1.06212189e-02  4.72839773e-02\n",
            "  3.07458988e-03  5.69180660e-02 -4.10046102e-03 -1.81440059e-02\n",
            " -8.41794014e-02  3.99755016e-02  7.82730654e-02  1.22335233e-01\n",
            " -3.30587812e-02 -6.11829609e-02 -4.00689989e-03  3.63230263e-03\n",
            " -1.24400975e-02 -2.38595475e-02  4.39472087e-02 -2.90459599e-02\n",
            "  9.46077518e-03  2.60384046e-02  6.91812392e-03  8.29010550e-03\n",
            " -8.49701539e-02  9.59865376e-03  5.34007959e-02  4.70132343e-02\n",
            " -5.96876964e-02 -8.56544450e-02 -1.93813965e-02  2.65980465e-03\n",
            "  1.93646352e-03  2.47262381e-02  1.63790155e-02  4.78571765e-02\n",
            "  2.93782149e-02  4.41442169e-02  1.08241299e-02 -5.83359711e-02\n",
            "  4.77030128e-02 -1.92043241e-02 -3.43958512e-02 -5.14776073e-02]\n",
            "[ 1.49584133e-02 -4.16007191e-02  6.26096204e-02  1.94953792e-02\n",
            " -1.13907205e-02 -5.42600304e-02  3.81554365e-02  3.79498154e-02\n",
            " -8.90221745e-02 -3.27782445e-02 -5.25734238e-02 -2.70171557e-02\n",
            " -2.51845662e-02 -1.91382095e-02  7.11073801e-02  1.20982770e-02\n",
            "  5.75343780e-02 -6.17451258e-02 -1.11238554e-01  4.14023958e-02\n",
            " -1.78370178e-01  9.62967351e-02  2.68420801e-02  1.87211148e-02\n",
            " -4.71457765e-02  5.66860382e-03 -1.56133091e-02  9.12312325e-03\n",
            "  1.07328735e-01  2.03227699e-02 -6.60899580e-02  2.28676535e-02\n",
            " -3.25866751e-02 -2.69889310e-02 -5.96747771e-02  5.83626479e-02\n",
            " -1.35824317e-02 -1.05727971e-01  4.95147221e-02  4.12711315e-02\n",
            "  1.10347162e-03 -2.21116431e-02 -2.82409359e-02  5.74893458e-03\n",
            "  5.70084117e-02 -6.13561599e-04  4.65752790e-03  1.51395919e-02\n",
            "  9.65222437e-03 -5.57120182e-02 -1.11079499e-01  6.73286766e-02\n",
            " -3.53063829e-02  4.64913361e-02  6.43088296e-02  7.08535239e-02\n",
            "  1.22954231e-03  4.40159487e-03  7.35190213e-02 -3.00576491e-03\n",
            "  4.26993035e-02  9.05284379e-03  3.10205054e-02  2.58741770e-02\n",
            "  5.96535131e-02 -6.85217092e-03 -5.71660623e-02 -5.07020764e-03\n",
            " -4.54535186e-02  6.32418180e-03 -2.04736218e-02 -8.78204927e-02\n",
            "  3.14842863e-03  1.40036670e-02  9.56610590e-03 -1.49078546e-02\n",
            "  3.18535604e-02 -1.88143849e-02  1.06350444e-01  5.33806160e-02\n",
            "  5.48402146e-02 -4.56660539e-02 -3.50029394e-02 -3.08206473e-02\n",
            " -7.88484439e-02  6.35045639e-04  5.79979829e-02  3.88477347e-03\n",
            " -2.56757028e-02  5.71687110e-02  6.22213371e-02  6.33096322e-02\n",
            "  4.56642881e-02  4.31264453e-02  4.65566106e-02 -3.82412225e-02\n",
            "  7.68882781e-02 -3.88619374e-03 -1.12093724e-01  1.67255610e-01\n",
            "  1.12293698e-02  1.64530259e-02  2.10918789e-03 -5.40895984e-02\n",
            " -3.29611613e-03  2.28487346e-02 -2.76970826e-02  6.16828874e-02\n",
            " -4.52501439e-02 -6.48819432e-02 -2.08259132e-02 -8.91133491e-03\n",
            "  2.14739349e-02  1.26174703e-01  2.54581831e-02 -5.41127101e-03\n",
            " -2.21863668e-02  2.51940563e-02  4.73069251e-02  3.85046867e-03\n",
            " -1.16417576e-02 -1.39246667e-02  2.42144261e-02  4.74519879e-02\n",
            " -4.61419532e-03 -3.29726301e-02  3.55132781e-02 -7.01789376e-33\n",
            "  1.06865034e-01  5.31705515e-03  6.88985437e-02  7.86328688e-02\n",
            " -6.74805865e-02  1.16485860e-02  2.30888948e-02 -4.20062942e-03\n",
            " -1.42891686e-02 -2.61168946e-02  5.07205799e-02  2.22489480e-02\n",
            " -6.78136945e-02  6.11229166e-02 -4.64679021e-03  2.77649891e-02\n",
            "  2.10689958e-02 -2.68871654e-02 -1.56514589e-02  1.01474643e-01\n",
            " -3.30441743e-02 -8.59582201e-02  1.47005646e-02  1.51335001e-02\n",
            "  1.62753407e-02  5.50026409e-02  6.41967803e-02 -9.01263133e-02\n",
            "  8.47516302e-03  2.00504605e-02 -7.68548027e-02  6.55234652e-03\n",
            " -2.61717523e-03 -4.02072147e-02  1.47181926e-02 -1.01261370e-01\n",
            " -1.90763921e-02  1.55695621e-02 -4.43501957e-02  3.88231836e-02\n",
            " -5.31870015e-02 -7.76810711e-03  3.33946645e-02 -3.50793526e-02\n",
            " -2.86047123e-02 -2.06188718e-03 -7.93400779e-02  6.75432803e-03\n",
            " -9.42519158e-02 -4.24090102e-02 -8.43186975e-02 -4.46394831e-02\n",
            " -7.45199844e-02 -3.93888466e-02 -4.09141667e-02 -3.79117057e-02\n",
            " -3.55928019e-02  1.13177314e-01 -1.37834130e-02  5.98610491e-02\n",
            "  4.09544520e-02  1.03296183e-01 -1.02248177e-01  6.36180351e-03\n",
            " -1.30003259e-01  4.41197306e-02  3.41789909e-02 -5.46083972e-02\n",
            "  3.31812091e-02 -6.01149686e-02  3.92224826e-03 -2.21871305e-02\n",
            "  3.78167666e-02  7.72622004e-02  1.03664696e-02  2.78206132e-02\n",
            " -1.70933660e-02  1.66352894e-02 -5.78599647e-02  2.80074077e-03\n",
            "  2.58818064e-02  4.55417447e-02 -3.98501381e-02 -1.57117173e-02\n",
            "  8.98282751e-02  3.41558754e-02 -1.06062025e-01 -2.46270094e-02\n",
            " -1.22622112e-02 -1.65420938e-02  1.54086920e-02  4.16846424e-02\n",
            "  5.29455282e-02 -4.01199050e-02 -9.46398601e-02  5.57671454e-33\n",
            "  1.21272072e-01  6.85684085e-02 -3.87836359e-02  3.61370631e-02\n",
            " -6.26359656e-02 -5.35657257e-02 -2.59997509e-02  4.84231412e-02\n",
            " -7.03730956e-02  7.72824883e-02  7.03286054e-03  3.17382291e-02\n",
            "  4.86429185e-02 -2.15126742e-02  5.13717905e-02 -2.80595031e-02\n",
            "  7.36738667e-02  1.06250569e-02 -2.59821340e-02 -1.58159696e-02\n",
            " -5.75096235e-02  1.01039290e-01 -5.97374178e-02  6.51665777e-02\n",
            " -2.68521141e-02  2.63223592e-02 -1.76277310e-02 -4.44545671e-02\n",
            " -5.08486032e-02 -2.29984242e-02  1.94733795e-02 -1.12984374e-01\n",
            " -7.43507221e-02 -2.07736716e-02  8.67058262e-02  4.22040522e-02\n",
            " -1.41101079e-02 -2.56742276e-02 -8.80077332e-02 -7.11958185e-02\n",
            "  1.39170401e-02  6.54527992e-02  1.82456523e-03  7.53626749e-02\n",
            "  4.96167764e-02 -2.22703107e-02  5.82451671e-02 -1.64823495e-02\n",
            "  4.74945344e-02  1.20642953e-01 -5.54021336e-02 -9.14550051e-02\n",
            " -2.35732812e-02  3.76479626e-02  5.74431159e-02  1.61601063e-02\n",
            " -3.42951789e-02 -6.04103804e-02 -1.35529824e-02 -6.34598434e-02\n",
            " -9.54229236e-02 -5.46458084e-03 -5.46359690e-03  5.21385558e-02\n",
            "  1.01913832e-01 -3.61306928e-02 -9.46241319e-02 -4.95159477e-02\n",
            " -3.18123996e-02 -4.86178882e-02  7.76494946e-03 -4.31911908e-02\n",
            "  3.60771758e-03 -6.31887615e-02  3.33929732e-02  1.40943816e-02\n",
            " -2.76108775e-02  4.92498130e-02 -3.19591537e-02  4.70430264e-03\n",
            " -1.69641897e-02  8.53325352e-02  8.39068666e-02 -2.86971349e-02\n",
            " -2.00241003e-02 -8.59680697e-02  6.60698190e-02  4.64325063e-02\n",
            "  2.87762471e-02 -4.18465072e-03  2.18970221e-04  2.73218118e-02\n",
            " -1.81723997e-04 -3.35065350e-02  2.78662387e-02 -1.56655382e-08\n",
            "  1.32071888e-02  1.04320357e-02 -3.61435562e-02 -8.61927037e-05\n",
            " -4.14362997e-02  5.25609497e-03 -3.97806279e-02  2.16395073e-02\n",
            " -5.13830036e-02  3.72514687e-03  4.29347437e-03 -2.20751092e-02\n",
            " -5.65212518e-02 -7.92329535e-02  7.88967907e-02  2.98105385e-02\n",
            "  6.60821004e-03  4.69332486e-02 -2.93195676e-02 -2.61830003e-03\n",
            "  1.72261726e-02 -4.26643295e-03  3.41666676e-02  5.54830916e-02\n",
            "  2.29102466e-02  6.57452121e-02 -1.30913425e-02 -6.78299516e-02\n",
            " -8.60351026e-02  4.54258285e-02  5.19598871e-02  6.96408674e-02\n",
            " -8.42224248e-03 -4.25159298e-02  3.04932054e-02 -1.97436884e-02\n",
            "  1.68936830e-02  3.09725525e-03  8.62893984e-02 -4.07531485e-02\n",
            " -1.52435889e-02  7.60531351e-02 -1.95212681e-02  4.38559540e-02\n",
            " -1.78623516e-02  5.10296924e-03  8.65336433e-02  7.85578117e-02\n",
            "  4.75807115e-03 -6.89911395e-02 -4.06064577e-02  1.28341634e-02\n",
            "  3.47046591e-02  2.80092694e-02 -1.12486556e-02  5.70688993e-02\n",
            "  2.03010514e-02  3.20966877e-02  6.66264212e-03 -5.48879914e-02\n",
            "  8.45868662e-02  2.25277545e-04 -3.46845901e-03 -8.05019662e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vector of some incomprehensible numbers are displayed. It is normal and sane not to make any sense of it. Like we did in Gensim yesterday, let's try to reduce the dimension of the sentences and see them on a graph."
      ],
      "metadata": {
        "id": "m_ZhEALXt2fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use PCA (Principal Component Analysis - fully mathematical and not at all boring) to reduce the dimensions.\n",
        "def display_pca_scatterplot(sentence1, sentence2):\n",
        "\n",
        "    sentence_vectors = np.array([sentence1, sentence2])\n",
        "\n",
        "    twodim = PCA().fit_transform(sentence_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')"
      ],
      "metadata": {
        "id": "tXdl0IE2Pj34"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "display_pca_scatterplot(embedding1, embedding2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "rESYQSoQQzm0",
        "outputId": "6f3c8ee4-b62f-4da4-e221-742c70e32bea"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty far for very reasonably similar sentences. Maybe the scale on both axes of the graphs may differ. So this didn't help.\n",
        "\n",
        "<br>\n",
        "\n",
        "To calculate the similarity, researchers tried a lot of calculations on the embeddings. How would you determine if two points on a space is close? It's obvious. We find the distance between them."
      ],
      "metadata": {
        "id": "p8Enc8bOuZCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have taken the magnitude of difference of the two embedding vectors - essentially calculating its distance,\n",
        "# or more mathematically called 'Euclidean distance'.\n",
        "dist = np.linalg.norm(embedding1 - embedding2)\n",
        "dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va44xqViSKMo",
        "outputId": "93b17e5b-3f37-470f-87a2-42b826b4b8cf"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6877326"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially, the researchers did find it exciting and applied it as a measure to find similar sentences, but it did not provide the accuracy we needed. We had to find the similarities **semantically.**\n",
        "\n",
        "<br>\n",
        "\n",
        "The next and by far the best method to calculate that, is _cosine similarity._\n",
        "\n",
        "<br>\n",
        "\n",
        "The cosine similarity determines the cosine of the angle between the two vectors. As said, this is a mathematical calculation done on a computer so even if you cannot visualize a 768-dimensional vector, you can still compute it."
      ],
      "metadata": {
        "id": "3krWh9_tvRxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = util.cos_sim(embedding1, embedding2)\n",
        "print(\"Cosine-Similarity:\", cosine_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpT7WOBkRr7W",
        "outputId": "9da60ed3-90bf-43cc-ebc6-cd4d1e8dcc59"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine-Similarity: tensor([[0.7635]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now seen how SentenceTransformer is used. Now, let us get inside and analyze how it works. Before that, it is important to know how this came to existence.\n",
        "\n",
        "The initial approach to finding similar sentences using BERT was to use a cross-encoder. In this approach, we feed both sentences into BERT model and we use the [CLS] token of the result to calculate the similarity output. But this proved computationally very intensive.\n",
        "\n",
        "<br>\n",
        "\n",
        "SentenceBERT uses a siamese BERT architecture, meaning, we find encoder representations for each token in each sentence, do a pooling process to extract the sentence embedding, and apply cosine_sim to it. This is computationally less intensive and also showed good enough accuracy.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's tear it apart and sew it back up."
      ],
      "metadata": {
        "id": "X9RvdROTx8pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences we want sentence embeddings for\n",
        "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2Q0xlX6ni3H",
        "outputId": "9c7beb59-4611-483b-acb2-6be4a288fd96"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is an example sentence', 'Each sentence is converted']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "Q_YO3zYLng4f"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sentences\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "encoded_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z53TTRNmnpHW",
        "outputId": "36144138-7afb-409f-bca9-5f4d1a3bf904"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
              "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "model_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phYJrRYsnrQ7",
        "outputId": "4c28336f-0a07-4669-9382-51ccd2332a21"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0366, -0.0162,  0.1682,  ...,  0.0554, -0.1644, -0.2967],\n",
              "         [ 0.7239,  0.6399,  0.1888,  ...,  0.5946,  0.6206,  0.4897],\n",
              "         [ 0.0064,  0.0203,  0.0448,  ...,  0.3464,  1.3170, -0.1670],\n",
              "         ...,\n",
              "         [ 0.1479, -0.0643,  0.1457,  ...,  0.8837, -0.3316,  0.2975],\n",
              "         [ 0.5212,  0.6563,  0.5607,  ..., -0.0399,  0.0412, -1.4036],\n",
              "         [ 1.0824,  0.7140,  0.3986,  ..., -0.2301,  0.3243, -1.0313]],\n",
              "\n",
              "        [[ 0.2802,  0.1165, -0.0418,  ...,  0.2711, -0.1685, -0.2961],\n",
              "         [ 0.8729,  0.4545, -0.1091,  ...,  0.1365,  0.4580, -0.2042],\n",
              "         [ 0.4752,  0.5731,  0.6304,  ...,  0.6526,  0.5612, -1.3268],\n",
              "         ...,\n",
              "         [ 0.6113,  0.7920, -0.4685,  ...,  0.0854,  1.0592, -0.2983],\n",
              "         [ 0.4115,  1.0946,  0.2385,  ...,  0.8984,  0.3684, -0.7333],\n",
              "         [ 0.1374,  0.5555,  0.2678,  ...,  0.5426,  0.4665, -0.5284]]]), pooler_output=tensor([[ 1.3429e-02,  4.0036e-02,  3.0797e-03,  7.7095e-03, -8.5741e-02,\n",
              "         -3.2874e-02,  4.5395e-02,  5.4421e-02, -6.6219e-02, -3.3736e-02,\n",
              "         -7.4499e-03,  3.3775e-02, -1.8523e-02, -1.2477e-02, -6.1699e-02,\n",
              "          7.9306e-02,  9.3979e-02, -2.9625e-02, -1.4692e-02,  5.6033e-02,\n",
              "          1.1484e-02,  1.1056e-02,  2.2872e-02, -2.9034e-02, -1.8242e-02,\n",
              "          1.3069e-01, -2.4484e-02,  5.1790e-02,  3.6784e-02,  8.1075e-02,\n",
              "          8.6604e-02,  3.3904e-04, -6.8685e-02,  3.2757e-02,  2.5933e-03,\n",
              "         -4.3434e-02, -1.7191e-02,  8.2270e-02, -4.7277e-02, -3.3682e-02,\n",
              "          6.5672e-02,  2.9311e-02, -5.9559e-02,  7.0777e-02, -1.5764e-02,\n",
              "          2.1118e-02, -1.0806e-01, -3.4479e-02, -4.8619e-02,  4.2684e-02,\n",
              "         -1.2006e-01,  4.6357e-02,  1.5641e-02, -9.6469e-03, -3.7291e-02,\n",
              "          3.7824e-02,  3.4880e-02, -1.5224e-02,  5.4148e-02,  2.4613e-02,\n",
              "          1.5847e-02, -2.4202e-02, -2.2713e-03, -7.8123e-03, -9.2186e-02,\n",
              "          4.1878e-03, -5.9705e-02, -1.0853e-01, -3.1122e-02, -2.0779e-02,\n",
              "         -3.8755e-02,  2.9059e-02, -4.6478e-02, -2.9519e-03,  2.0831e-02,\n",
              "          5.2982e-02, -4.9873e-02,  2.5957e-02, -6.8848e-02, -9.3444e-04,\n",
              "          9.7473e-03,  3.8063e-02,  6.7740e-02, -1.2013e-01, -4.9044e-02,\n",
              "         -7.2494e-02, -5.4891e-02,  4.2310e-02, -7.8360e-03,  7.4343e-02,\n",
              "          4.2601e-02,  1.1231e-02, -1.3430e-01,  5.6984e-02,  8.3218e-02,\n",
              "         -2.5391e-02, -6.5197e-03, -5.7111e-02, -1.7962e-01, -4.5549e-03,\n",
              "          7.8273e-02, -9.1693e-03, -2.6726e-02,  2.8545e-02, -2.9945e-03,\n",
              "         -8.1081e-02, -1.1999e-02,  7.7119e-02, -2.2280e-02,  3.5329e-02,\n",
              "          9.1938e-02,  2.5805e-02,  4.1517e-02,  1.5394e-02, -4.7167e-02,\n",
              "          7.5881e-02, -1.8906e-02, -3.8819e-02, -1.1037e-01,  6.3771e-02,\n",
              "          1.2736e-01, -4.3087e-02,  6.0566e-02,  4.5662e-02,  1.2671e-02,\n",
              "          1.6947e-03,  4.5216e-02, -6.5698e-02, -9.2475e-02, -4.4247e-02,\n",
              "          1.1738e-01, -4.1540e-02,  9.1604e-02, -1.2345e-01, -6.4776e-02,\n",
              "          5.9695e-02, -4.8307e-02, -1.9186e-02, -4.8370e-02,  8.4520e-02,\n",
              "          6.1380e-02, -1.3651e-01,  4.2905e-02, -3.0625e-02,  7.8972e-02,\n",
              "          1.2373e-02, -7.8566e-02, -1.1315e-01,  2.2296e-03, -2.0005e-02,\n",
              "          7.4801e-02, -7.9967e-02, -9.2751e-02,  4.6135e-02,  4.7488e-03,\n",
              "          1.9638e-02,  3.5892e-02, -2.1168e-02, -1.0492e-02, -5.3379e-02,\n",
              "         -4.8980e-02, -7.6467e-02,  8.6475e-02,  1.7718e-02, -5.9111e-02,\n",
              "          2.3288e-02,  2.2532e-02, -3.4328e-02, -2.9752e-02, -6.2304e-02,\n",
              "         -8.1938e-02,  1.7710e-02, -4.8280e-02, -6.2486e-02,  3.1141e-02,\n",
              "          2.3507e-02,  5.2302e-02, -6.9293e-02,  1.1098e-02, -2.0867e-02,\n",
              "          5.7609e-02, -1.0762e-01, -6.3090e-02, -3.2820e-02,  5.2514e-02,\n",
              "          3.2342e-02,  3.6938e-02,  7.0675e-02,  1.0392e-01, -5.7706e-02,\n",
              "          1.6733e-02,  1.4690e-02,  2.7245e-02,  5.0453e-02, -8.0111e-03,\n",
              "         -3.9295e-02, -7.9695e-02,  1.6183e-02,  2.3606e-02,  1.4193e-02,\n",
              "          1.6472e-02,  7.4927e-02, -4.1723e-03,  4.2714e-02, -1.2629e-02,\n",
              "         -2.7271e-02, -6.2421e-02, -1.4004e-01, -2.9299e-02,  1.6869e-02,\n",
              "          6.9726e-02,  2.7200e-02, -4.3012e-02, -3.9393e-02, -6.9088e-02,\n",
              "         -5.2691e-02,  8.2750e-02,  7.5312e-02,  8.0191e-02, -1.2825e-02,\n",
              "          4.1995e-02, -1.2516e-02,  2.7372e-02, -1.2028e-01, -1.2387e-01,\n",
              "         -5.4954e-02, -6.5397e-03, -6.6795e-02,  1.3161e-02,  2.2269e-02,\n",
              "         -2.9861e-02,  7.6297e-02,  2.7364e-02,  1.0883e-01, -8.8141e-03,\n",
              "          8.6819e-02, -2.4719e-02, -4.9682e-02,  5.1936e-02,  3.4862e-02,\n",
              "         -1.2001e-01,  8.3150e-02, -2.0191e-02, -5.3333e-03, -7.3319e-02,\n",
              "          4.7377e-02,  6.8626e-02,  5.7092e-02,  1.0385e-02, -3.3254e-02,\n",
              "          1.5860e-02,  1.5843e-02,  3.2043e-03,  3.3453e-02, -2.3585e-02,\n",
              "          5.4750e-02,  2.8557e-02, -2.5185e-02,  2.1314e-02,  3.8470e-02,\n",
              "          4.0113e-02,  1.6145e-02, -5.4575e-02, -1.4631e-03, -4.4623e-02,\n",
              "          2.8726e-02,  1.1302e-01, -4.1902e-02, -1.3932e-02, -9.6624e-02,\n",
              "          1.8770e-02,  9.0820e-03, -8.1532e-02, -8.8747e-03,  1.0511e-01,\n",
              "          2.2679e-02,  2.5884e-02, -7.2527e-02,  6.1241e-02, -1.5532e-02,\n",
              "          1.4980e-02,  5.6270e-02,  6.3009e-02, -4.2786e-02,  7.4515e-02,\n",
              "         -2.7271e-02,  4.7316e-02, -2.8739e-02,  4.9152e-02,  8.2691e-02,\n",
              "          1.2656e-02, -2.7052e-02, -4.3005e-02,  9.0674e-03, -5.3151e-02,\n",
              "          6.2785e-02,  3.9300e-02,  6.7608e-02, -8.9390e-03,  3.2900e-02,\n",
              "         -3.2308e-02, -3.5465e-02,  7.5596e-02, -2.9816e-02, -3.6290e-02,\n",
              "          1.5477e-02, -4.8453e-02,  7.6023e-03,  3.5700e-02,  1.4346e-02,\n",
              "          8.0476e-02,  6.1430e-02,  5.6743e-03,  1.1575e-02, -4.4946e-02,\n",
              "         -3.3531e-02,  1.4398e-02, -1.3222e-01, -4.2706e-03, -3.7443e-02,\n",
              "         -1.4443e-02, -7.8702e-02,  2.7131e-02,  6.0028e-02,  5.8509e-02,\n",
              "         -1.2207e-02,  2.7320e-02,  3.1169e-02, -4.1544e-02, -2.1317e-02,\n",
              "          1.6517e-02, -2.7584e-02,  3.4797e-02,  4.2613e-02,  1.3636e-03,\n",
              "         -8.5100e-02,  3.7460e-02,  4.3458e-02, -9.4549e-03,  3.9982e-02,\n",
              "          4.2519e-02, -8.8832e-02, -7.8078e-03, -1.1596e-01,  9.5078e-02,\n",
              "         -9.7206e-02,  6.5254e-02,  2.9393e-02,  3.7238e-03, -2.1312e-02,\n",
              "          1.9388e-02, -5.0150e-02,  6.9220e-02, -1.0495e-02, -2.4837e-02,\n",
              "         -7.5547e-02, -2.7963e-02, -2.5091e-03,  6.8878e-02, -4.4469e-02,\n",
              "         -3.9632e-02, -1.1323e-02, -5.2578e-02, -8.7117e-02,  2.7642e-02,\n",
              "         -4.9512e-02,  2.0677e-02, -1.1490e-02,  1.2818e-02,  2.9686e-02,\n",
              "         -1.0459e-01,  5.0408e-02,  1.0962e-01,  4.6272e-02, -1.1507e-04,\n",
              "          6.5767e-03, -8.2945e-02,  5.0269e-02, -5.0819e-02, -6.9119e-02,\n",
              "          6.4439e-02,  3.8906e-02, -6.8270e-02, -9.6023e-03],\n",
              "        [-1.6412e-02,  2.0522e-02,  2.4394e-02,  7.2660e-02, -1.1512e-01,\n",
              "          2.3330e-02,  4.0078e-03,  5.9773e-02, -5.6792e-02,  1.5403e-02,\n",
              "         -1.9216e-04,  1.2416e-02,  4.9173e-02, -4.5729e-03, -1.0675e-01,\n",
              "          6.8074e-02,  1.2190e-01, -8.6178e-03, -2.9459e-02,  9.0053e-02,\n",
              "         -6.7227e-03,  1.7838e-02,  1.7992e-02, -6.7095e-02, -2.6161e-02,\n",
              "          1.1410e-01,  1.9777e-03, -7.3730e-03,  4.2695e-02,  6.5730e-02,\n",
              "          9.1121e-02,  2.3741e-02, -8.6613e-02,  3.9649e-02, -2.5507e-02,\n",
              "         -2.1669e-02, -6.7568e-02,  1.0122e-01, -2.1715e-02,  2.3547e-03,\n",
              "          8.2525e-02,  4.1297e-02, -7.3701e-02,  7.1083e-02, -1.2322e-02,\n",
              "         -4.2826e-02, -1.2831e-01,  1.0527e-02,  5.1972e-04,  1.2830e-02,\n",
              "         -1.3040e-01,  8.9542e-02,  8.1203e-02, -6.0886e-03, -6.3051e-02,\n",
              "          5.1941e-02,  5.5326e-02,  1.4283e-02,  4.2129e-02,  2.1932e-02,\n",
              "         -1.1775e-02, -3.3309e-02, -3.3257e-02, -5.0021e-02, -9.2995e-02,\n",
              "          8.0433e-03, -6.2652e-02, -9.0668e-02, -2.9693e-02,  1.3278e-02,\n",
              "         -1.2384e-02,  2.4325e-02, -1.6497e-02, -1.5386e-02,  7.7987e-03,\n",
              "          8.2918e-02, -3.9625e-02,  3.0166e-03, -7.1665e-02,  3.2242e-02,\n",
              "         -3.1861e-02,  5.1786e-02,  7.8073e-02, -1.3790e-01, -5.6443e-02,\n",
              "         -8.6384e-02, -1.8559e-02,  9.2650e-02, -8.3894e-03,  6.5256e-02,\n",
              "          4.7546e-03, -4.9196e-02, -1.5398e-01,  3.6895e-02,  1.2527e-01,\n",
              "         -5.4036e-02, -9.4263e-03, -2.3183e-02, -2.2138e-01,  2.2979e-03,\n",
              "          8.8022e-02, -6.1781e-02, -6.3716e-02,  3.4578e-02,  2.7633e-02,\n",
              "         -6.0287e-02, -4.5254e-03,  4.8531e-02,  2.7681e-02, -1.8568e-02,\n",
              "          1.1042e-01, -3.9830e-03,  3.4688e-02,  4.7291e-02, -4.1807e-02,\n",
              "          3.1925e-02, -3.4659e-02, -7.9173e-02, -1.3209e-01,  1.0513e-01,\n",
              "          1.0988e-01, -5.0374e-02,  9.4562e-03,  5.7217e-02,  2.4318e-02,\n",
              "         -2.5906e-02,  7.1816e-02, -4.5563e-02, -7.4738e-02, -7.7397e-02,\n",
              "          1.6668e-01, -2.5206e-02,  7.5146e-02, -8.2775e-02, -8.3848e-02,\n",
              "          6.7262e-02, -2.4362e-02,  5.4102e-03, -1.7812e-02,  7.4477e-02,\n",
              "          5.5941e-02, -1.7995e-01,  4.4545e-02, -5.4162e-02,  4.8428e-02,\n",
              "         -3.0975e-03, -8.8899e-02, -7.1491e-02, -8.5257e-03, -6.4380e-02,\n",
              "          1.2850e-01, -6.2238e-02, -8.5901e-02,  4.5637e-02, -2.5258e-02,\n",
              "          2.0246e-02,  6.9027e-02, -2.5333e-02,  2.0954e-02, -9.4911e-02,\n",
              "         -7.4532e-02, -9.7445e-02,  5.3053e-02,  6.4805e-02, -1.7467e-02,\n",
              "          7.8287e-03,  5.3832e-02,  1.7221e-02, -4.1689e-02, -3.5192e-02,\n",
              "         -5.9480e-02, -2.4315e-03, -5.2786e-02, -7.3131e-02,  3.8923e-02,\n",
              "          2.5390e-02,  5.8073e-02, -1.1277e-01, -1.3115e-02,  3.7546e-03,\n",
              "          5.8880e-02, -8.6683e-02, -1.1336e-01, -5.9326e-02,  1.5321e-02,\n",
              "          3.7837e-02,  2.3076e-02,  3.6856e-02,  1.1715e-01, -7.1100e-02,\n",
              "          5.0757e-03, -6.6640e-04,  6.2937e-02,  6.0390e-02, -3.4757e-02,\n",
              "         -3.7994e-02, -1.0356e-01,  4.6444e-03, -5.8601e-04, -3.8506e-02,\n",
              "         -1.8700e-03,  6.6083e-02, -4.0170e-02,  3.9136e-02,  5.1422e-02,\n",
              "         -1.2792e-02, -1.2741e-01, -1.1371e-01, -7.4030e-03,  5.0568e-02,\n",
              "          8.8432e-02,  4.5259e-02, -2.2429e-02, -4.3520e-02, -9.5628e-02,\n",
              "         -5.2261e-02,  1.1536e-01,  8.3722e-02,  1.0216e-01, -1.2807e-02,\n",
              "          2.7735e-02, -1.3866e-02,  2.4131e-03, -7.8098e-02, -8.1450e-02,\n",
              "         -4.9948e-02, -2.2054e-02, -7.6649e-02,  2.2537e-02,  1.5444e-02,\n",
              "         -2.4304e-02,  1.0686e-01,  5.5686e-02,  1.2883e-01, -2.3435e-02,\n",
              "          1.1903e-01, -7.6298e-03, -3.7377e-02,  9.4343e-02,  3.0764e-02,\n",
              "         -1.2493e-01,  1.4389e-01, -5.6773e-02, -1.0457e-02, -6.1964e-02,\n",
              "          7.7382e-02,  1.1770e-01,  3.2609e-02,  3.9971e-02, -3.2292e-02,\n",
              "          3.0921e-02, -2.9455e-03, -3.9535e-02,  1.6244e-02, -1.2068e-02,\n",
              "          7.4872e-02,  2.4877e-02, -6.3048e-02,  8.2138e-02,  5.6135e-02,\n",
              "          2.2832e-02, -1.5446e-02, -5.2837e-02,  1.2114e-02,  1.3555e-02,\n",
              "          1.9292e-02,  9.3866e-02, -4.8043e-02, -4.0721e-02, -1.0090e-01,\n",
              "          6.4863e-02,  1.0930e-02, -1.3199e-01, -3.3183e-02,  7.5213e-02,\n",
              "          3.0600e-02,  9.9645e-03, -9.6374e-02,  7.0916e-02, -1.1066e-02,\n",
              "          2.9517e-02,  4.1136e-02,  3.9592e-02, -6.6500e-02,  4.8719e-02,\n",
              "         -7.3052e-03,  4.9139e-02, -2.4039e-02,  8.4684e-02,  1.5190e-01,\n",
              "         -5.5680e-03, -1.6845e-02, -1.8212e-02,  4.7395e-02, -1.0036e-01,\n",
              "          8.4382e-02,  4.0205e-02,  8.6075e-02, -2.0358e-02,  1.8569e-02,\n",
              "         -6.9477e-02, -4.6497e-02,  9.4527e-02, -5.3387e-02, -4.0131e-02,\n",
              "         -2.2027e-02, -8.5622e-02,  1.8590e-02,  3.0398e-02,  2.2257e-03,\n",
              "          6.8550e-02,  6.2411e-02,  5.4371e-03,  7.3998e-02,  1.7568e-02,\n",
              "          6.2321e-03,  6.0236e-02, -1.4625e-01,  4.7108e-03, -3.3739e-02,\n",
              "         -3.1894e-03, -4.1378e-02,  3.4950e-02,  5.7236e-02,  4.0846e-02,\n",
              "         -2.7780e-02,  5.6281e-02,  5.4090e-02,  1.2975e-02, -1.8945e-03,\n",
              "          3.3885e-02, -8.3884e-02,  7.6936e-02,  5.1811e-02,  4.6538e-02,\n",
              "         -1.0081e-01,  1.3060e-01,  2.5463e-02,  4.1129e-02,  7.5722e-02,\n",
              "          3.9883e-02, -8.1436e-02, -2.9125e-02, -9.2279e-02,  1.0999e-01,\n",
              "         -8.7998e-02,  1.7189e-02,  2.2057e-02,  5.7705e-04, -1.8651e-03,\n",
              "          2.7676e-02,  1.9341e-03,  6.4815e-02,  1.2107e-02, -4.4955e-02,\n",
              "         -1.0582e-01, -4.7512e-02,  3.1620e-02,  2.9339e-02,  2.8688e-02,\n",
              "         -1.5993e-02, -2.3867e-02, -6.3935e-02, -9.3625e-02,  5.4746e-02,\n",
              "         -4.6345e-02,  1.2985e-03, -2.4204e-02,  3.1631e-02,  3.0533e-02,\n",
              "         -9.0704e-02,  7.9624e-02,  9.8593e-02,  3.1205e-02, -1.6831e-02,\n",
              "          7.8870e-03, -1.1406e-01,  1.2890e-01, -5.1266e-02, -9.4784e-02,\n",
              "          3.2232e-02,  3.0980e-02, -9.9598e-02, -6.3203e-03]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where the next layer of operations are done. We perform a pooling operation, where we take the vectors of each token and multiply it with its attention mask. We then divide the resulting vector by the sum of attention vectors to produce the final sentence embedding. (This method is called mean pooling - I wonder why)"
      ],
      "metadata": {
        "id": "m17NsN3B0LXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() #Change dimensions of attention mask for multiplication with the matrix.\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1)/ torch.clamp(input_mask_expanded.sum(1), min=1e-9) # torch.clamp exists for a reason, think and tell me why. Don't Google."
      ],
      "metadata": {
        "id": "JDgGXklbndcn"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "O_cX7EBtmvFv"
      },
      "outputs": [],
      "source": [
        "# Perform pooling\n",
        "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize embeddings\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)"
      ],
      "metadata": {
        "id": "GLrDdcognx2e"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence embeddings:\")\n",
        "print(sentence_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWmbklysn37Q",
        "outputId": "34e17ca9-3523-4a36-cead-c9a99034858b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence embeddings:\n",
            "tensor([[ 6.7657e-02,  6.3496e-02,  4.8713e-02,  7.9305e-02,  3.7448e-02,\n",
            "          2.6528e-03,  3.9375e-02, -7.0985e-03,  5.9361e-02,  3.1537e-02,\n",
            "          6.0098e-02, -5.2905e-02,  4.0607e-02, -2.5931e-02,  2.9843e-02,\n",
            "          1.1269e-03,  7.3515e-02, -5.0382e-02, -1.2239e-01,  2.3703e-02,\n",
            "          2.9727e-02,  4.2477e-02,  2.5634e-02,  1.9952e-03, -5.6919e-02,\n",
            "         -2.7160e-02, -3.2904e-02,  6.6025e-02,  1.1901e-01, -4.5879e-02,\n",
            "         -7.2621e-02, -3.2584e-02,  5.2341e-02,  4.5055e-02,  8.2531e-03,\n",
            "          3.6702e-02, -1.3942e-02,  6.5392e-02, -2.6427e-02,  2.0641e-04,\n",
            "         -1.3664e-02, -3.6281e-02, -1.9504e-02, -2.8974e-02,  3.9427e-02,\n",
            "         -8.8409e-02,  2.6243e-03,  1.3671e-02,  4.8306e-02, -3.1157e-02,\n",
            "         -1.1733e-01, -5.1169e-02, -8.8529e-02, -2.1896e-02,  1.4299e-02,\n",
            "          4.4417e-02, -1.3482e-02,  7.4339e-02,  2.6638e-02, -1.9876e-02,\n",
            "          1.7919e-02, -1.0605e-02, -9.0426e-02,  2.1327e-02,  1.4120e-01,\n",
            "         -6.4717e-03, -1.4038e-03, -1.5361e-02, -8.7357e-02,  7.2217e-02,\n",
            "          2.0140e-02,  4.2559e-02, -3.4901e-02,  3.1951e-04, -8.0297e-02,\n",
            "         -3.2747e-02,  2.8527e-02, -5.1366e-02,  1.0939e-01,  8.1933e-02,\n",
            "         -9.8404e-02, -9.3410e-02, -1.5129e-02,  4.5125e-02,  4.9417e-02,\n",
            "         -2.5187e-02,  1.5708e-02, -1.2929e-01,  5.3189e-03,  4.0234e-03,\n",
            "         -2.3457e-02, -6.7298e-02,  2.9228e-02, -2.6085e-02,  1.3063e-02,\n",
            "         -3.1166e-02, -4.8271e-02, -5.5886e-02, -3.8750e-02,  1.2001e-01,\n",
            "         -1.0392e-02,  4.8970e-02,  5.5354e-02,  4.4936e-02, -4.0098e-03,\n",
            "         -1.0296e-01, -2.9297e-02, -5.8340e-02,  2.7047e-02, -2.2017e-02,\n",
            "         -7.2224e-02, -4.1387e-02, -1.9330e-02,  2.7333e-03,  2.7700e-04,\n",
            "         -9.6759e-02, -1.0057e-01, -1.4192e-02, -8.0789e-02,  4.5392e-02,\n",
            "          2.4504e-02,  5.9761e-02, -7.3818e-02,  1.1984e-02, -6.6340e-02,\n",
            "         -7.6905e-02,  3.8516e-02, -5.5936e-33,  2.8001e-02, -5.6078e-02,\n",
            "         -4.8660e-02,  2.1557e-02,  6.0198e-02, -4.8140e-02, -3.5025e-02,\n",
            "          1.9331e-02, -1.7515e-02, -3.8921e-02, -3.8107e-03, -1.7029e-02,\n",
            "          2.8210e-02,  1.2829e-02,  4.7160e-02,  6.2103e-02, -6.4359e-02,\n",
            "          1.2929e-01, -1.3123e-02,  5.2307e-02, -3.7368e-02,  2.8909e-02,\n",
            "         -1.6898e-02, -2.3733e-02, -3.3349e-02, -5.1676e-02,  1.5536e-02,\n",
            "          2.0880e-02, -1.2537e-02,  4.5958e-02,  3.7272e-02,  2.8057e-02,\n",
            "         -5.9001e-02, -1.1699e-02,  4.9218e-02,  4.7033e-02,  7.3549e-02,\n",
            "         -3.7053e-02,  3.9846e-03,  1.0641e-02, -1.6148e-04, -5.2717e-02,\n",
            "          2.7593e-02, -3.9292e-02,  8.4472e-02,  4.8686e-02, -4.8587e-03,\n",
            "          1.7995e-02, -4.2857e-02,  1.2338e-02,  6.3995e-03,  4.0482e-02,\n",
            "          1.4889e-02, -1.5394e-02,  7.6295e-02,  2.3704e-02,  4.4524e-02,\n",
            "          5.0820e-02, -2.3125e-03, -1.8874e-02, -1.2334e-02,  4.6600e-02,\n",
            "         -5.6344e-02,  6.2993e-02, -3.1554e-02,  3.2491e-02,  2.3467e-02,\n",
            "         -6.5544e-02,  2.0171e-02,  2.5708e-02, -1.2387e-02, -8.3649e-03,\n",
            "         -6.6438e-02,  9.4307e-02, -3.5709e-02, -3.4248e-02, -6.6636e-03,\n",
            "         -8.0153e-03, -3.0971e-02,  4.3301e-02, -8.2140e-03, -1.5079e-01,\n",
            "          3.0769e-02,  4.0072e-02, -3.7929e-02,  1.9322e-03,  4.0053e-02,\n",
            "         -8.7708e-02, -3.6849e-02,  8.5796e-03, -3.1925e-02, -1.2526e-02,\n",
            "          7.3554e-02,  1.3473e-03,  2.0592e-02,  2.7110e-33, -5.1858e-02,\n",
            "          5.7836e-02, -9.1898e-02,  3.9442e-02,  1.0558e-01, -1.9691e-02,\n",
            "          6.1840e-02, -7.6347e-02,  2.4088e-02,  9.4005e-02, -1.1654e-01,\n",
            "          3.7120e-02,  5.2243e-02, -3.9586e-03,  5.7221e-02,  5.3285e-03,\n",
            "          1.2402e-01,  1.3902e-02, -1.1025e-02,  3.5605e-02, -3.3075e-02,\n",
            "          8.1657e-02, -1.5200e-02,  6.0559e-02, -6.0140e-02,  3.2610e-02,\n",
            "         -3.4830e-02, -1.6988e-02, -9.7491e-02, -2.7148e-02,  1.7471e-03,\n",
            "         -7.6898e-02, -4.3186e-02, -1.8998e-02, -2.9166e-02,  5.7749e-02,\n",
            "          2.4182e-02, -1.1690e-02, -6.2144e-02,  2.8435e-02, -2.3752e-04,\n",
            "         -2.5178e-02,  4.3964e-03,  8.1284e-02,  3.6418e-02, -6.0401e-02,\n",
            "         -3.6552e-02, -7.9375e-02, -5.0853e-03,  6.6970e-02, -1.1778e-01,\n",
            "          3.2374e-02, -4.7125e-02, -1.3446e-02, -9.4844e-02,  8.2496e-03,\n",
            "         -1.0675e-02, -6.8188e-02,  1.1182e-03,  2.4802e-02, -6.3589e-02,\n",
            "          2.8449e-02, -2.6130e-02,  8.5811e-02,  1.1468e-01, -5.3535e-02,\n",
            "         -5.6359e-02,  4.2601e-02,  1.0945e-02,  2.0958e-02,  1.0013e-01,\n",
            "          3.2605e-02, -1.8421e-01, -3.9321e-02, -6.9145e-02, -6.3810e-02,\n",
            "         -6.5639e-02, -6.4125e-03, -4.7961e-02, -7.6813e-02,  2.9538e-02,\n",
            "         -2.2995e-02,  4.1704e-02, -2.5005e-02, -4.5450e-03, -4.1714e-02,\n",
            "         -1.3229e-02, -6.3836e-02, -2.4647e-03, -1.3734e-02,  1.6898e-02,\n",
            "         -6.3040e-02,  8.9888e-02,  4.1817e-02, -1.8569e-02, -1.8044e-08,\n",
            "         -1.6800e-02, -3.2158e-02,  6.3038e-02, -4.1309e-02,  4.4482e-02,\n",
            "          2.0246e-03,  6.2959e-02, -5.1737e-03, -1.0044e-02, -3.0564e-02,\n",
            "          3.5267e-02,  5.5858e-02, -4.6712e-02,  3.4510e-02,  3.2958e-02,\n",
            "          4.3011e-02,  2.9436e-02, -3.0316e-02, -1.7111e-02,  7.3748e-02,\n",
            "         -5.4791e-02,  2.7752e-02,  6.2017e-03,  1.5880e-02,  3.4298e-02,\n",
            "         -5.1575e-03,  2.3508e-02,  7.5314e-02,  1.9284e-02,  3.3620e-02,\n",
            "          5.0910e-02,  1.5250e-01,  1.6421e-02,  2.7053e-02,  3.7516e-02,\n",
            "          2.1855e-02,  5.6633e-02, -3.9575e-02,  7.1231e-02, -5.4138e-02,\n",
            "          1.0376e-03,  2.1185e-02, -3.5631e-02,  1.0902e-01,  2.7653e-03,\n",
            "          3.1400e-02,  1.3842e-03, -3.4574e-02, -4.5928e-02,  2.8808e-02,\n",
            "          7.1690e-03,  4.8469e-02,  2.6102e-02, -9.4407e-03,  2.8217e-02,\n",
            "          3.4872e-02,  3.6910e-02, -8.5895e-03, -3.5321e-02, -2.4786e-02,\n",
            "         -1.9192e-02,  3.8071e-02,  5.9965e-02, -4.2229e-02],\n",
            "        [ 8.6439e-02,  1.0276e-01,  5.3946e-03,  2.0445e-03, -9.9633e-03,\n",
            "          2.5385e-02,  4.9288e-02, -3.0627e-02,  6.8725e-02,  1.0137e-02,\n",
            "          7.7540e-02, -9.0081e-02,  6.1062e-03, -5.6990e-02,  1.4171e-02,\n",
            "          2.8049e-02, -8.6846e-02,  7.6440e-02, -1.0349e-01, -6.7744e-02,\n",
            "          6.9995e-02,  8.4425e-02, -7.2491e-03,  1.0477e-02,  1.3402e-02,\n",
            "          6.7758e-02, -9.4209e-02, -3.7169e-02,  5.2262e-02, -3.1085e-02,\n",
            "         -9.6341e-02,  1.5772e-02,  2.5787e-02,  7.8525e-02,  7.8995e-02,\n",
            "          1.9152e-02,  1.6436e-02,  3.1009e-03,  3.8131e-02,  2.3709e-02,\n",
            "          1.0539e-02, -4.4064e-02,  4.4174e-02, -2.5873e-02,  6.1538e-02,\n",
            "         -4.0543e-02, -8.6414e-02,  3.1972e-02, -8.9065e-04, -2.4444e-02,\n",
            "         -9.1972e-02,  2.3394e-02, -8.3029e-02,  4.4151e-02, -2.4969e-02,\n",
            "          6.2302e-02, -1.3035e-03,  7.5140e-02,  2.4639e-02, -6.4724e-02,\n",
            "         -1.1773e-01,  3.8339e-02, -9.1177e-02,  6.3545e-02,  7.6274e-02,\n",
            "         -8.8024e-02,  9.5456e-03, -4.6972e-02, -8.4174e-02,  3.8882e-02,\n",
            "         -1.1439e-01,  6.2886e-03, -3.4936e-02,  2.3975e-02, -3.3132e-02,\n",
            "         -1.5724e-02, -3.7896e-02, -8.8125e-03,  7.0612e-02,  3.2807e-02,\n",
            "          2.0367e-03, -1.1228e-01,  6.7972e-03,  1.2277e-02,  3.3530e-02,\n",
            "         -1.3620e-02, -2.2549e-02, -2.2523e-02, -2.0319e-02,  5.0430e-02,\n",
            "         -7.4865e-02, -8.2282e-02,  7.6596e-02,  4.9339e-02, -3.7555e-02,\n",
            "          1.4463e-02, -5.7246e-02, -1.7995e-02,  1.0970e-01,  1.1946e-01,\n",
            "          8.0925e-04,  6.1706e-02,  3.2632e-02, -1.3078e-01, -1.4864e-01,\n",
            "         -6.1623e-02,  4.3389e-02,  2.6713e-02,  1.3979e-02, -3.9400e-02,\n",
            "         -2.5271e-02,  3.8774e-03,  3.5866e-02, -6.1542e-02,  3.7666e-02,\n",
            "          2.6757e-02, -3.8266e-02, -3.5479e-02, -2.3923e-02,  8.6798e-02,\n",
            "         -1.8406e-02,  7.7104e-02,  1.3987e-03,  7.0038e-02, -4.7788e-02,\n",
            "         -7.8982e-02,  5.1081e-02, -2.9987e-33, -3.9165e-02, -2.5621e-03,\n",
            "          1.6521e-02,  9.4894e-03, -5.6622e-02,  6.5778e-02, -4.7700e-02,\n",
            "          1.1166e-02, -5.7356e-02, -9.1626e-03, -2.1752e-02, -5.5953e-02,\n",
            "         -1.1142e-02,  9.3279e-02,  1.6677e-02, -1.3672e-02,  4.3439e-02,\n",
            "          1.8724e-03,  7.2995e-03,  5.1633e-02,  4.8061e-02,  1.3534e-01,\n",
            "         -1.7174e-02, -1.2970e-02, -7.5011e-02,  2.6111e-02,  2.6980e-02,\n",
            "          7.8306e-04, -4.8727e-02,  1.1784e-02, -4.5958e-02, -4.8321e-02,\n",
            "         -1.9567e-02,  1.9389e-02,  1.9881e-02,  1.6743e-02,  9.8780e-02,\n",
            "         -2.7409e-02,  2.3481e-02,  3.7023e-03, -6.1451e-02, -1.2123e-03,\n",
            "         -9.5047e-03,  9.2515e-03,  2.3844e-02,  8.6123e-02,  2.2679e-02,\n",
            "          5.4512e-04,  3.4713e-02,  6.2546e-03, -6.9278e-03,  3.9240e-02,\n",
            "          1.1567e-02,  3.2628e-02,  6.2216e-02,  2.7611e-02,  1.8688e-02,\n",
            "          3.5581e-02,  4.1180e-02,  1.5478e-02,  4.2269e-02,  3.8225e-02,\n",
            "          1.0031e-02, -2.8325e-02,  4.4705e-02, -4.1046e-02, -4.5055e-03,\n",
            "         -5.4473e-02,  2.6232e-02,  1.7986e-02, -1.2312e-01, -4.6695e-02,\n",
            "         -1.3591e-02,  6.4671e-02,  3.5735e-03, -1.2223e-02, -1.7938e-02,\n",
            "         -2.5550e-02,  2.3722e-02,  4.0866e-03, -6.5148e-02,  4.4365e-02,\n",
            "          4.6860e-02, -3.2517e-02,  4.0226e-03, -3.9761e-03,  1.1194e-02,\n",
            "         -9.9560e-02,  3.3317e-02,  8.0106e-02,  9.4269e-02, -6.3829e-02,\n",
            "          3.2315e-02, -5.1355e-02, -7.4988e-03,  5.3005e-34, -4.1320e-02,\n",
            "          9.4965e-02, -1.0640e-01,  4.9659e-02, -3.4191e-02, -3.1675e-02,\n",
            "         -1.7156e-02,  1.7010e-03,  5.7976e-02, -1.2178e-03, -1.6854e-02,\n",
            "         -5.1691e-02,  5.5300e-02, -3.4265e-02,  3.0818e-02, -3.1048e-02,\n",
            "          9.2753e-02,  3.7266e-02, -2.3740e-02,  4.4589e-02,  1.4615e-02,\n",
            "          1.1624e-01, -5.0011e-02,  3.8872e-02,  4.2474e-03,  2.5698e-02,\n",
            "          3.2724e-02,  4.2991e-02, -1.3614e-02,  2.5612e-02,  1.0626e-02,\n",
            "         -8.4686e-02, -9.5298e-02,  1.0840e-01, -7.5160e-02, -1.3777e-02,\n",
            "          6.3734e-02, -4.4967e-03, -3.2532e-02,  6.2361e-02,  3.4805e-02,\n",
            "         -3.5492e-02, -2.0022e-02,  3.6661e-02, -2.4884e-02,  1.0182e-02,\n",
            "         -7.0123e-02, -4.3195e-02,  2.9533e-02, -2.9490e-04, -3.4539e-02,\n",
            "          1.4668e-02, -9.8397e-02, -4.7049e-02, -8.8550e-03, -8.8991e-02,\n",
            "          3.5100e-02, -1.2960e-01, -4.9887e-02, -6.1205e-02, -5.9780e-02,\n",
            "          9.4632e-03,  4.9122e-02, -7.7503e-02,  8.0973e-02, -4.7926e-02,\n",
            "          2.3438e-03,  7.5703e-02, -2.4018e-02, -1.5255e-02,  4.8674e-02,\n",
            "         -3.8597e-02, -7.0483e-02, -1.2035e-02, -3.8879e-02, -7.7602e-02,\n",
            "         -1.0724e-02,  1.0419e-02, -2.1375e-02, -9.1739e-02, -1.1134e-02,\n",
            "         -2.9607e-02,  2.4646e-02,  4.6571e-03, -1.6345e-02, -3.9522e-02,\n",
            "          7.7337e-02, -2.8473e-02, -3.6994e-03,  8.2767e-02, -1.1041e-02,\n",
            "          3.1398e-02,  5.3509e-02,  5.7515e-02, -3.1762e-02, -1.5291e-08,\n",
            "         -7.9966e-02, -4.7680e-02, -8.5979e-02,  5.6962e-02, -4.0887e-02,\n",
            "          2.2383e-02, -4.6445e-03, -3.8013e-02, -3.1067e-02, -1.0728e-02,\n",
            "          1.9770e-02,  7.7700e-03, -6.0947e-03, -3.8638e-02,  2.8027e-02,\n",
            "          6.7814e-02, -2.3535e-02,  3.2175e-02,  8.0254e-03, -2.3911e-02,\n",
            "         -1.2200e-03,  3.1460e-02, -5.2492e-02, -8.0682e-03,  3.1478e-03,\n",
            "          5.1150e-02, -4.4410e-02,  6.3601e-02,  3.8508e-02,  3.3043e-02,\n",
            "         -4.1873e-03,  4.9559e-02, -5.6961e-02, -6.4971e-03, -2.4979e-02,\n",
            "         -1.6087e-02,  6.6229e-02, -2.0631e-02,  1.0805e-01,  1.6855e-02,\n",
            "          1.4381e-02, -1.3213e-02, -1.2939e-01,  6.9522e-02, -5.5577e-02,\n",
            "         -6.7541e-02, -5.4582e-03, -6.1359e-03,  3.9084e-02, -6.2878e-02,\n",
            "          3.7406e-02, -1.1657e-02,  1.2915e-02, -5.5250e-02,  5.1608e-02,\n",
            "         -4.3084e-03,  5.8025e-02,  1.8694e-02,  2.2781e-02,  3.2167e-02,\n",
            "          5.3798e-02,  7.0285e-02,  7.4931e-02, -8.4178e-02]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores = util.cos_sim(sentence_embeddings[0],sentence_embeddings[1])\n",
        "cosine_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGKQYE1SdxfV",
        "outputId": "4340ff18-301d-4a92-db14-85c87c34b819"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4046]])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see what is an extension of sentence similarity. In a given set of sentences, we try to find the two most similar sentences and display it."
      ],
      "metadata": {
        "id": "drg6-hE010tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single list of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']"
      ],
      "metadata": {
        "id": "CvRp5VS8o7vf"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the models\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.cos_sim(embeddings, embeddings)"
      ],
      "metadata": {
        "id": "N46Uz3Xdo8o7"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    for j in range(i+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})"
      ],
      "metadata": {
        "id": "xbMwHbm4o_a_"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "df = pd.DataFrame(columns = ['Sentence_1','Sentence_2','Cosine_Sim'])\n",
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    df = df.append({'Sentence_1':sentences[i],'Sentence_2':sentences[j], 'Cosine_Sim':pair['score']}, ignore_index = True)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "lbTJUA-UoQet",
        "outputId": "f6fa6325-89d2-423d-9ee0-53727602a4a2"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Sentence_1                   Sentence_2      Cosine_Sim\n",
              "0     The new movie is awesome    The new movie is so great  tensor(0.8939)\n",
              "1         The cat sits outside  The cat plays in the garden  tensor(0.6788)\n",
              "2                 I love pasta           Do you like pizza?  tensor(0.5096)\n",
              "3                 I love pasta    The new movie is so great  tensor(0.2560)\n",
              "4                 I love pasta     The new movie is awesome  tensor(0.2440)\n",
              "5      A man is playing guitar  The cat plays in the garden  tensor(0.2105)\n",
              "6     The new movie is awesome           Do you like pizza?  tensor(0.1969)\n",
              "7    The new movie is so great           Do you like pizza?  tensor(0.1692)\n",
              "8         The cat sits outside           A woman watches TV  tensor(0.1310)\n",
              "9  The cat plays in the garden           Do you like pizza?  tensor(0.0900)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3272a39-2a01-4771-8aa1-7f6fd4243ba7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence_1</th>\n",
              "      <th>Sentence_2</th>\n",
              "      <th>Cosine_Sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The new movie is awesome</td>\n",
              "      <td>The new movie is so great</td>\n",
              "      <td>tensor(0.8939)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The cat sits outside</td>\n",
              "      <td>The cat plays in the garden</td>\n",
              "      <td>tensor(0.6788)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I love pasta</td>\n",
              "      <td>Do you like pizza?</td>\n",
              "      <td>tensor(0.5096)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I love pasta</td>\n",
              "      <td>The new movie is so great</td>\n",
              "      <td>tensor(0.2560)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I love pasta</td>\n",
              "      <td>The new movie is awesome</td>\n",
              "      <td>tensor(0.2440)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>A man is playing guitar</td>\n",
              "      <td>The cat plays in the garden</td>\n",
              "      <td>tensor(0.2105)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The new movie is awesome</td>\n",
              "      <td>Do you like pizza?</td>\n",
              "      <td>tensor(0.1969)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The new movie is so great</td>\n",
              "      <td>Do you like pizza?</td>\n",
              "      <td>tensor(0.1692)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The cat sits outside</td>\n",
              "      <td>A woman watches TV</td>\n",
              "      <td>tensor(0.1310)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The cat plays in the garden</td>\n",
              "      <td>Do you like pizza?</td>\n",
              "      <td>tensor(0.0900)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3272a39-2a01-4771-8aa1-7f6fd4243ba7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3272a39-2a01-4771-8aa1-7f6fd4243ba7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3272a39-2a01-4771-8aa1-7f6fd4243ba7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think this is pretty cool. Imagine, if we are given a huge corpus with billions of documents with such free text. If someone gives a query, we can try to find the most similar web document related to the query.\n",
        "\n",
        "Lo and behold, we just came up with Google, dingus.\n",
        "\n",
        "Execute the cell below to see it in action. (Here instead of documents, we use a list of sentences for your understanding)."
      ],
      "metadata": {
        "id": "z447UO6b2Hea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'A man is riding a horse.',\n",
        "          'A woman is playing violin.',\n",
        "          'Two men pushed carts through the woods.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'A cheetah is running behind its prey.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Query sentences:\n",
        "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\n",
        "\n",
        "\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = min(5, len(corpus))\n",
        "for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
        "\n",
        "# [FUN FACT] : Since 2020, almost every single English language query you enter\n",
        "# in Google is powered by BERT. I told you. It's everywhere."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "155svlaQouYf",
        "outputId": "382406dc-2ddc-47f1-c2a8-5ec2f2f80140"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: A man is eating pasta.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A man is eating food. (Score: 0.7035)\n",
            "A man is eating a piece of bread. (Score: 0.5272)\n",
            "A man is riding a horse. (Score: 0.1889)\n",
            "A man is riding a white horse on an enclosed ground. (Score: 0.1047)\n",
            "A cheetah is running behind its prey. (Score: 0.0980)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Someone in a gorilla costume is playing a set of drums.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A monkey is playing drums. (Score: 0.6433)\n",
            "A woman is playing violin. (Score: 0.2564)\n",
            "A man is riding a horse. (Score: 0.1389)\n",
            "A man is riding a white horse on an enclosed ground. (Score: 0.1191)\n",
            "A cheetah is running behind its prey. (Score: 0.1080)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: A cheetah chases prey on across a field.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A cheetah is running behind its prey. (Score: 0.8253)\n",
            "A man is eating food. (Score: 0.1399)\n",
            "A monkey is playing drums. (Score: 0.1292)\n",
            "A man is riding a white horse on an enclosed ground. (Score: 0.1097)\n",
            "A man is riding a horse. (Score: 0.0650)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we have it, the end of sentence similarity. And maybe the secret to powering a global conglomerate."
      ],
      "metadata": {
        "id": "M9FQS7wd24oZ"
      }
    }
  ]
}